{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15RUG95utk-i"
      },
      "outputs": [],
      "source": [
        "# import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPY6-HO93MAF",
        "outputId": "72fb00f6-3259-4b11-b2d9-7d465a6625ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/DeepLearningProj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkAVIL3Pj9",
        "outputId": "13d6f531-fbb7-49b4-ef8d-c82a217a1b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DeepLearningProj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g8DLRRzEuXs"
      },
      "source": [
        "import string\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.backend import set_session\n",
        "import keras\n",
        "import sys, time, os, warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from PIL import Image\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.utils import shuffle\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWzOpdwJIpil"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# To remove punctuations\n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(string.punctuation)\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "\n",
        "# To remove single characters\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "# To remove numeric values\n",
        "def remove_numeric(text,printTF=False):\n",
        "    text_no_numeric = \"\"\n",
        "    for word in text.split():\n",
        "        isalpha = word.isalpha()\n",
        "        if printTF:\n",
        "            print(\"    {:10} : {:}\".format(word,isalpha))\n",
        "        if isalpha:\n",
        "            text_no_numeric += \" \" + word\n",
        "    return(text_no_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnpEMWQ6dlA6"
      },
      "source": [
        "from os import listdir\n",
        "\n",
        "## The location of the caption file\n",
        "descriptions_dir = 'Flickr_8ktoken.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qvrlLpxRoDG",
        "outputId": "ada0f000-f3f5-403f-f39d-7a8c87b1f861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## read in the Flickr caption data\n",
        "text = load_doc(descriptions_dir)\n",
        "print(text[:325])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset2(text):\n",
        "  df = []\n",
        "  for sentences in text.split('\\n'):\n",
        "      \n",
        "      df.append(sentences)\n",
        "  return df"
      ],
      "metadata": {
        "id": "0Cd_q9Q5FaxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = make_dataset2(text)"
      ],
      "metadata": {
        "id": "3lVOO26fEnva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[:-1]"
      ],
      "metadata": {
        "id": "9WWtrpAxHD8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "  \n",
        "  df[i] = str(df[i]).split('\\t')[1]"
      ],
      "metadata": {
        "id": "YBnaD_dzGkAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    #text = remove_numeric(text)\n",
        "    return(text)\n",
        "    \n",
        "for i, caption in enumerate(df):\n",
        "    df[i] = text_clean(caption)\n"
      ],
      "metadata": {
        "id": "VeYrNK1cIier"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "i = 0\n",
        "while i < len(df)-2:\n",
        "  text_pairs.append((df[i],\"[start] \"+df[i+2]+\" [end]\"))\n",
        "  text_pairs.append((df[i+1],\"[start] \"+df[i+3]+\" [end]\"))\n",
        "  text_pairs.append((df[i+1],\"[start] \"+df[i+4]+\" [end]\"))\n",
        "  i = i + 5\n"
      ],
      "metadata": {
        "id": "EGhUElG4I5wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsBAo0B6tk-o"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBSVVR-dtk-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad5076f-4b4e-403f-de01-e3598ae45168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(' large tan dog is rolling around on its back in yard of grass', '[start]  Large dog rolling on its back in green grass [end]')\n",
            "(' man and woman are dressed as Groucho Marx with fake cigars', '[start]  woman and man pose with Groucho Marx disguises [end]')\n",
            "(' kid in an ocean rides wave', '[start]  young man playing in wave at the beach [end]')\n",
            "(' brown dog jumping through the grass', '[start]  dog running in fenced yard [end]')\n",
            "(' little girl is playing red recorder flute while another girl is playing pink guitar', '[start]  One girl is playing recorder while another one holds guitar [end]')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "  print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJHZJuEtk-r"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup For finetuning data**"
      ],
      "metadata": {
        "id": "3ci5I25KCtSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rom_dir = '/content/drive/MyDrive/DeepLearningProj/romantic_train.txt'\n",
        "funny_dir = '/content/drive/MyDrive/DeepLearningProj/funny_train.txt'"
      ],
      "metadata": {
        "id": "KUwSrO8U2Q6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'rb')\n",
        "\t# read all text\n",
        "\ttext = file.read().decode(errors='replace')\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "metadata": {
        "id": "dFuKjnsZ4Pwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rom_text = load_doc(rom_dir)\n",
        "funny_text = load_doc(funny_dir)\n",
        "print(rom_text[:325])\n",
        "print(\"==============\")\n",
        "print(funny_text[:325])\n",
        "# print(len(rom_text))\n",
        "# print(len(funny_text))"
      ],
      "metadata": {
        "id": "9u8Yf-rP36ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = rom_text.split(\"\\n\")\n",
        "print(len(lines))\n",
        "# text_pairs = []\n",
        "rom_sens=[]\n",
        "for line in lines:\n",
        "    rom= line.split(\"\\n\")\n",
        "    # rom = \"[start] \" + rom + \" [end]\"\n",
        "    # text_pairs.append((eng, spa))\n",
        "    rom_sens.append(rom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm6Yphv_59pW",
        "outputId": "8dde0d98-5084-4404-ed02-00329bce6331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = funny_text.split(\"\\n\")[:-2]\n",
        "print(len(lines))\n",
        "# text_pairs = []\n",
        "funny_sens=[]\n",
        "for line in lines:\n",
        "    funny= line.split(\"\\n\")\n",
        "    # spa = \"[start] \" + spa + \" [end]\"\n",
        "    # text_pairs.append((eng, spa))\n",
        "    funny_sens.append(funny)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6A7xvhx7U5V",
        "outputId": "f82212cf-a2cb-4bbf-c287-1b0490085f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "funny_sens[50][0].split('\\r')[0]\n",
        "# listToString(funny_sens[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xqi6ENGR7q41",
        "outputId": "341b6665-b9fc-4c86-f091-2e008ab158c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a man is standing on a snowcapped mountain trying to search for treasure .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rom_sens[50:55]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC1BMl0xBEYc",
        "outputId": "0842abbb-f1f2-4a24-bef7-df81f615e913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a person hikes down a snowy mountain to overcome their fear of height .\\r'],\n",
              " ['two dogs in love running through a yard with a forest . \\r'],\n",
              " ['a racecar driver spins tires, never give up .\\r'],\n",
              " ['a man peeks through a hole in the wall, hoping to save a life .\\r'],\n",
              " ['a brown dog steps into murky water , careful to swim back to his master .\\r']]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(string.punctuation)\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "\n",
        "# To remove single characters\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "    \n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(string.punctuation)\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "# To remove single characters\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    #text = remove_numeric(text)\n",
        "    return(text)\n",
        "    \n"
      ],
      "metadata": {
        "id": "Tc4dnnReWXph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(funny_sens)):\n",
        "    funny_sens[i] = text_clean(str(funny_sens[i]))\n",
        "\n",
        "for i in range(len(rom_sens)):\n",
        "    rom_sens[i] = text_clean(str(rom_sens[i]))"
      ],
      "metadata": {
        "id": "6jhOgG63dVx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs2=[]\n",
        "for i in range(len(rom_sens)):\n",
        "  funny_temp=\"[start] \"+(funny_sens[i].split('\\\\r')[0])[3:]+\" [end]\"\n",
        "  text_pairs2.append(((rom_sens[i].split('\\\\r')[0])[3:],funny_temp))"
      ],
      "metadata": {
        "id": "GnSQpP-DBOck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTrYyvy6tk-s"
      },
      "outputs": [],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.1 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTE-Q13Atk-v"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "rom_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "funny_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        ")\n",
        "train_rom_texts = [pair[0] for pair in train_pairs]\n",
        "train_funny_texts = [pair[1] for pair in train_pairs]\n",
        "#rom_vectorization.adapt(train_rom_texts)\n",
        "#funny_vectorization.adapt(train_funny_texts)\n",
        "\n",
        "text_rom_texts = [pair[0] for pair in text_pairs]\n",
        "text_funny_texts = [pair[1] for pair in text_pairs]\n",
        "text2_rom_texts = [pair[0] for pair in text_pairs2]\n",
        "text2_funny_texts = [pair[1] for pair in text_pairs2]\n",
        "text_combined = text2_rom_texts + text2_funny_texts + text_rom_texts + text_funny_texts\n",
        "rom_vectorization.adapt(text_combined)\n",
        "funny_vectorization.adapt(text_combined)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voc_f = funny_vectorization.get_vocabulary()\n",
        "word_index_f = dict(zip(voc_f, range(len(voc_f))))\n",
        "\n",
        "voc_r = rom_vectorization.get_vocabulary()\n",
        "word_index_r = dict(zip(voc_r, range(len(voc_r))))"
      ],
      "metadata": {
        "id": "bb-dR1HK5zsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc_f) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index_f.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "        \n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "id": "QyFz95xU0l6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer_f = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "btEkqy_v0o4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TLrkrUHtk-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_dataset(rom, funny):\n",
        "    rom = rom_vectorization(rom)\n",
        "    funny = funny_vectorization(funny)\n",
        "    return ({\"encoder_inputs\": rom, \"decoder_inputs\": funny[:, :-1],}, funny[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    rom_texts, funny_texts = zip(*pairs)\n",
        "    rom_texts = list(rom_texts)\n",
        "    funny_texts = list(funny_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((rom_texts, funny_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXLp3HKBtk-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90245ed6-ebb2-46ff-b0f1-11dbc324e4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \".glove.6B.100d.txt\"\n",
        ")\n",
        "path_to_glove_file = os.path.join(\"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "id": "FxFkJV4a0Q0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ6kvYqItk-0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.token_embeddings = Embedding(\n",
        "        num_tokens,\n",
        "        embedding_dim,\n",
        "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "        trainable=False,\n",
        "    )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4utDZ8P1tk-3"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder_outputs = layers.Dropout(0.2)(encoder_outputs)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvRG0Spftk-4"
      },
      "outputs": [],
      "source": [
        "epochs = 100  # This should be at least 30 for convergence 150 done already\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    tf.keras.optimizers.RMSprop(lr=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6, verbose=0, mode='max')\n",
        "# mcp_save = tf.keras.callbacks.ModelCheckpoint(p, save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "# reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=8, verbose=1, epsilon=1e-4, mode='max')\n",
        "history=transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[earlyStopping])#,callbacks=[earlyStopping,mcp_save,reduce_lr_loss]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#t1 original\n",
        "#t2 single char removed\n",
        "#t3 combined vectorization\n",
        "p=\"./model_Transfer3.h5\"\n",
        "transformer.save_weights(p)"
      ],
      "metadata": {
        "id": "V5Tdk5XBMT_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMuqNVeitk-6"
      },
      "outputs": [],
      "source": [
        "funny_vocab = funny_vectorization.get_vocabulary()\n",
        "funny_index_lookup = dict(zip(range(len(funny_vocab)), funny_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = rom_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = funny_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = funny_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"end\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_rom_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_rom_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence)\n",
        "    print(translated)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TModuleGrammarTraining",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}